import json
import os
import urlparse

from collections import OrderedDict
from datetime import datetime
from itertools import groupby
from operator import attrgetter

import requests

from django.conf import settings
from django.contrib.contenttypes.fields import GenericForeignKey, GenericRelation
from django.contrib.contenttypes.models import ContentType
from django.core.urlresolvers import reverse
from django.db import models
from django.dispatch import receiver

from raven.contrib.django.raven_compat.models import client

import account.signals

from common import fields
from common.utils import full_url
from public_data.models import PublicDataAccess


def get_upload_dir(datafile_model, user):
    """
    Construct the upload dir path for a given User and DataFile model.
    """
    return 'member/%s/imported-data/%s/' % (user.username,
                                            datafile_model._meta.app_label)


def get_upload_path(instance, filename=''):
    """
    Construct the upload path for a given DataFile and filename.
    """
    return '%s%s' % (get_upload_dir(type(instance),
                                    instance.user_data.user),
                     filename)


class DataRetrievalTaskQuerySet(models.QuerySet):
    """
    Convenience methods for filtering DataRetrievalTasks.
    """
    def for_user(self, user):
        return self.filter(user=user).order_by('-start_time')

    @staticmethod
    def most_recent(tasks):
        """
        Return the most recent task with files if there are any, and the
        most recent task if not.
        """
        with_files = [task for task in tasks if task.data_files]

        if with_files:
            return with_files[0]

        return tasks[0]

    def grouped_recent(self):
        """
        Return a dict where each key is the name of a source and each value is
        the latest task for that source.
        """
        get_source = attrgetter('source')

        sorted_tasks = sorted(self, key=get_source)
        grouped_tasks = groupby(sorted_tasks, key=get_source)

        groups = {}

        for key, group in grouped_tasks:
            groups[key] = self.most_recent(list(group))

        return groups

    # Filter these in Python rather than in SQL so we can reuse the query cache
    # rather than hit the database each time
    def normal(self):
        return [task for task in self
                if task.status not in [DataRetrievalTask.TASK_FAILED,
                                       DataRetrievalTask.TASK_POSTPONED]]

    def postponed(self):
        return [task for task in self
                if task.status == DataRetrievalTask.TASK_POSTPONED]

    def failed(self):
        return [task for task in self
                if task.status == DataRetrievalTask.TASK_FAILED]


class DataRetrievalTask(models.Model):
    """
    Model for tracking DataFile import requests.

    The datafile_model field stores a ContentType referring to the app-specific
    DataFile model appropriate for files (if any) generated by the task.

    A DataRetrievalTask is related to DataFile models as a ForeignKey.

    Fields:
        status          (IntegerField): Task status, choices defined by
                        self.TASK_STATUS_CHOICES
        start_time      (DateTimeField): Time task was sent to processing.
        complete_time   (DateTimeField): Time task reported as complete/failed.
        datafile_model  (ForeignKey): ContentType for DataFile model used for
                        files (if any) created by this task's data import.
        user            (ForeignKey): User that requested this import task.
        app_task_params (TextField): JSON string with app-specific task params,
                        e.g. sample/user IDs. Default is blank.
    """
    objects = DataRetrievalTaskQuerySet.as_manager()

    TASK_SUCCEEDED = 0   # Celery task complete, successful.
    TASK_SUBMITTED = 1   # Sent to Open Humans Data Processing.
    TASK_FAILED = 2      # Celery task complete, failed.
    TASK_QUEUED = 3      # OH Data Processing has sent to broker.
    TASK_INITIATED = 4   # Celery has received and started the task.
    TASK_POSTPONED = 5   # Task not submitted yet (eg pending email validation)

    TASK_STATUS_CHOICES = OrderedDict(
        [(TASK_SUCCEEDED, 'Completed successfully'),
         (TASK_SUBMITTED, 'Submitted'),
         (TASK_FAILED, 'Failed'),
         (TASK_QUEUED, 'Queued'),
         (TASK_INITIATED, 'Initiated'),
         (TASK_POSTPONED, 'Postponed')])

    status = models.IntegerField(choices=TASK_STATUS_CHOICES.items(),
                                 default=TASK_SUBMITTED)
    start_time = models.DateTimeField(default=datetime.now)
    complete_time = models.DateTimeField(null=True)
    datafile_model = models.ForeignKey(ContentType)
    user = models.ForeignKey(settings.AUTH_USER_MODEL)
    app_task_params = models.TextField(default='')

    # Order reverse chronologically by default
    class Meta:
        ordering = ['-start_time']

    def __unicode__(self):
        return '%s:%s:%s' % (self.user,
                             self.source,
                             self.TASK_STATUS_CHOICES[self.status])

    @property
    def data_files(self):
        return (self.datafile_model.get_all_objects_for_this_type()
                .filter(task=self))

    @property
    def has_any_public_data_files(self):
        return (self.data_files.filter(
            _public_data_access__is_public=True).count() > 0)

    @property
    def source(self):
        return self.datafile_model.model_class()._meta.app_label

    def start_task(self):
        # Target URL is automatically determined from relevant app label.
        task_url = urlparse.urljoin(
            settings.DATA_PROCESSING_URL,
            self.datafile_model.model_class()._meta.app_label)

        try:
            task_req = requests.get(
                task_url,
                params={'task_params': json.dumps(self.get_task_params())})
        except requests.exceptions.RequestException:
            print 'Error in sending request to data processing'
            print self.get_task_params()

            error_message = 'Error in call to Open Humans Data Processing.'

        if 'task_req' in locals() and not task_req.status_code == 200:
            print 'Non-200 response from request sent to data processing'
            print self.get_task_params()

            error_message = 'Open Humans Data Processing not returning 200.'

        if 'error_message' in locals():
            # Note: could change later if processing works anyway
            self.status = self.TASK_FAILED
            self.save()

            client.captureMessage(error_message,
                                  error_data=self.__base_task_params())

    def postpone_task(self):
        self.status = self.TASK_POSTPONED
        self.save()

    def get_task_params(self):
        params = json.loads(self.app_task_params)
        params.update(self.__base_task_params())
        return params

    def __base_task_params(self):
        """
        Task parameters all tasks use. Subclasses may not override.
        """
        s3_key_dir = get_upload_dir(self.datafile_model.model_class(),
                                    self.user)
        s3_bucket_name = settings.AWS_STORAGE_BUCKET_NAME

        return {
            'member_id': self.user.member.member_id,
            's3_key_dir': s3_key_dir,
            's3_bucket_name': s3_bucket_name,
            'task_id': self.id,
            'update_url': full_url('/data-import/task-update/'),
        }


@receiver(account.signals.email_confirmed)
def start_postponed_tasks_cb(email_address, **kwargs):
    """
    A signal that starts any postponed address when a user's email is
    confirmed.
    """
    postponed_tasks = (DataRetrievalTask.objects.for_user(email_address.user)
                       .postponed())

    for task in postponed_tasks:
        task.start_task()


def delete_file(instance, **kwargs):  # pylint: disable=unused-argument
    """
    Delete the DataFile's file from S3 when the model itself is deleted.
    """
    instance.file.delete(save=False)


class BaseDataFileManager(models.Manager):
    """
    We use a manager so that subclasses of BaseDataFile also get their
    pre_delete signal connected correctly.
    """
    def contribute_to_class(self, model, name):
        super(BaseDataFileManager, self).contribute_to_class(model, name)

        models.signals.pre_delete.connect(delete_file, model)


class DataFileAccessLog(models.Model):
    """
    Represents a download of a datafile.
    """
    date = models.DateTimeField(auto_now_add=True)
    ip_address = models.GenericIPAddressField()
    user = models.ForeignKey(settings.AUTH_USER_MODEL, null=True)
    data_file = GenericForeignKey('data_file_model', 'data_file_id')
    data_file_model = models.ForeignKey(ContentType)
    data_file_id = models.PositiveIntegerField()

    def __unicode__(self):
        return '{} {} {} {}'.format(self.date, self.ip_address, self.user,
                                    self.data_file.file.url)


class BaseDataFile(models.Model):
    """
    Attributes that need to be defined in subclass:
        task:      ForeignKey to data_import.DataRetrievalTask with an
                   app-specific related_name argument.
        user_data: ForeignKey to an app-specific model (i.e. UserData) which
                   has a 'user' field that is a OneToOneField to User.
    """
    objects = BaseDataFileManager()

    file = models.FileField(upload_to=get_upload_path, max_length=1024)
    task = None
    user_data = None
    subtype = models.CharField(max_length=64, blank=True, null=True)

    class Meta:
        abstract = True

    def save(self, *args, **kwargs):
        if not self.subtype and hasattr(self, 'default_subtype'):
            self.subtype = self.default_subtype

        super(BaseDataFile, self).save(*args, **kwargs)

    def __unicode__(self):
        return '%s:%s:%s' % (self.user_data.user, self.source, self.file)

    @property
    def download_url(self):
        datafile_type = ContentType.objects.get(
            app_label=self._meta.app_label, model=self._meta.model_name)
        return reverse('data-management:datafile-download', args=[
            datafile_type.pk,
            self.id])

    @property
    def public_data_access(self):
        # TODO: determine if public sharing is enabled for the source.
        return False

    def has_access(self, user=None):
        if self.public_data_access:
            return True
        elif self.user == user:
            return True
        return False

    @property
    def source(self):
        return self._meta.app_label

    @property
    def basename(self):
        return os.path.basename(self.file.name)


class TestUserData(models.Model):
    """
    This is used for unit tests in public_data.tests; there's not currently a
    way to make test-specific model definitions in Django (a bug open since
    2009, #7835)
    """
    user = fields.AutoOneToOneField(settings.AUTH_USER_MODEL,
                                    related_name='test_user_data')


class TestDataFile(BaseDataFile):
    """
    Ditto the above model.
    """
    user_data = models.ForeignKey(TestUserData)
    task = models.ForeignKey(DataRetrievalTask,
                             related_name='datafile_test_data_file',
                             null=True, blank=True)
